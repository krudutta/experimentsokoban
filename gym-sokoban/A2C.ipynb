{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import gym\n",
    "import gym_sokoban\n",
    "\n",
    "# import os\n",
    "# __path__=[os.path.dirname(os.path.abspath(__file__))]\n",
    "\n",
    "import multiprocessing_env as mp \n",
    "import Sokoban as sb\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OnPolicy, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def act(self, x, deterministic=False):\n",
    "        logit, value = self.forward(x)\n",
    "        probs = F.softmax(logit)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = probs.max(1)[1]\n",
    "        else:\n",
    "            action = probs.multinomial(1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def evaluate_actions(self, x, action):\n",
    "        logit, value = self.forward(x)\n",
    "        \n",
    "        probs     = F.softmax(logit)\n",
    "        log_probs = F.log_softmax(logit)\n",
    "        \n",
    "        action_log_probs = log_probs.gather(1, action)\n",
    "        entropy = -(probs * log_probs).sum(1).mean()\n",
    "        \n",
    "        return logit, action_log_probs, value, entropy\n",
    "\n",
    "class ActorCritic(OnPolicy):\n",
    "    def __init__(self, in_shape, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0], 32, kernel_size=8, stride=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.critic  = nn.Linear(128, 1)\n",
    "        self.actor   = nn.Linear(128, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        logit = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logit, value\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1, -1).size(1)\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_envs, state_shape):\n",
    "        self.num_steps = num_steps\n",
    "        self.num_envs  = num_envs\n",
    "        self.states  = torch.zeros(num_steps + 1, num_envs, *state_shape)\n",
    "        self.rewards = torch.zeros(num_steps,     num_envs, 1)\n",
    "        self.masks   = torch.ones(num_steps  + 1, num_envs, 1)\n",
    "        self.actions = torch.zeros(num_steps,     num_envs, 1).long()\n",
    "        self.use_cuda = False\n",
    "            \n",
    "    def cuda(self):\n",
    "        self.use_cuda  = True\n",
    "        self.states    = self.states.cuda()\n",
    "        self.rewards   = self.rewards.cuda()\n",
    "        self.masks     = self.masks.cuda()\n",
    "        self.actions   = self.actions.cuda()\n",
    "        \n",
    "    def insert(self, step, state, action, reward, mask):\n",
    "        self.states[step + 1].copy_(state)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step + 1].copy_(mask)\n",
    "        \n",
    "    def after_update(self):\n",
    "        self.states[0].copy_(self.states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        \n",
    "    def compute_returns(self, next_value, gamma):\n",
    "        returns   = torch.zeros(self.num_steps + 1, self.num_envs, 1)\n",
    "        if self.use_cuda:\n",
    "            returns = returns.cuda()\n",
    "        returns[-1] = next_value\n",
    "        for step in reversed(range(self.num_steps)):\n",
    "            returns[step] = returns[step + 1] * gamma * self.masks[step + 1] + self.rewards[step]\n",
    "        return returns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
      "(3, 80, 80)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/ipykernel_launcher.py:92: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/ipykernel_launcher.py:97: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REWARDS=> MEAN: tensor(-11.5000) MAX: tensor(-11.0000) MEDIAN: tensor(-12.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/kru/Documents/Projects/Python Scripts/RL/sokobanv1/gym-sokoban/multiprocessing_env.py\", line 15, in worker\n",
      "    ob = env.reset()\n",
      "  File \"/home/kru/Documents/Projects/Python Scripts/RL/sokobanv1/gym-sokoban/Sokoban.py\", line 20, in reset\n",
      "    image = self.env.reset()\n",
      "  File \"/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/gym/wrappers/time_limit.py\", line 44, in reset\n",
      "    return self.env.reset()\n",
      "  File \"/home/kru/Documents/Projects/Python Scripts/RL/sokobanv1/gym-sokoban/gym_sokoban/envs/sokoban_tiny_world_env.py\", line 20, in reset\n",
      "    super(TinyWorldSokobanEnv, self).reset()\n",
      "  File \"/home/kru/Documents/Projects/Python Scripts/RL/sokobanv1/gym-sokoban/gym_sokoban/envs/sokoban_env.py\", line 161, in reset\n",
      "    num_boxes=self.num_boxes\n",
      "  File \"/home/kru/Documents/Projects/Python Scripts/RL/sokobanv1/gym-sokoban/gym_sokoban/envs/room_utils.py\", line 49, in generate_room\n",
      "    raise RuntimeWarning('Generated Model with score == 0')\n",
      "RuntimeWarning: Generated Model with score == 0\n"
     ]
    }
   ],
   "source": [
    "num_envs = 2\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = sb.SokobanEnv()\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = mp.SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape\n",
    "\n",
    "#a2c hyperparams:\n",
    "gamma = 0.99\n",
    "entropy_coef = 0.01\n",
    "value_loss_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "num_steps = 120\n",
    "num_batch = int(10e6)\n",
    "\n",
    "#rmsprop hyperparams:\n",
    "lr    = 7e-4\n",
    "eps   = 1e-5\n",
    "alpha = 0.99\n",
    "\n",
    "print(envs.observation_space.shape)\n",
    "#Init a2c and rmsprop\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
    "optimizer = optim.RMSprop(actor_critic.parameters(), lr, eps=eps, alpha=alpha)\n",
    "    \n",
    "if USE_CUDA:\n",
    "    actor_critic = actor_critic.cuda()\n",
    "\n",
    "rollout = RolloutStorage(num_steps, num_envs, envs.observation_space.shape)\n",
    "# rollout.cuda()\n",
    "\n",
    "all_rewards = []\n",
    "all_losses  = []\n",
    "\n",
    "state = envs.reset()\n",
    "state = torch.FloatTensor(np.float32(state))\n",
    "\n",
    "\n",
    "rollout.states[0].copy_(state)\n",
    "\n",
    "episode_rewards = torch.zeros(num_envs, 1)\n",
    "final_rewards   = torch.zeros(num_envs, 1)\n",
    "\n",
    "for i_update in range(num_batch):\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        action = actor_critic.act(Variable(state))\n",
    "        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())\n",
    "\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "        episode_rewards += reward\n",
    "        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1-masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "\n",
    "        if USE_CUDA:\n",
    "            masks = masks.cuda()\n",
    "\n",
    "        state = torch.FloatTensor(np.float32(next_state))\n",
    "        rollout.insert(step, state, action.data, reward, masks)\n",
    "\n",
    "\n",
    "    _, next_value = actor_critic(Variable(rollout.states[-1], requires_grad=True))\n",
    "    with torch.no_grad():\n",
    "        next_value = next_value.data\n",
    "\n",
    "    returns = rollout.compute_returns(next_value, gamma)\n",
    "\n",
    "    logit, action_log_probs, values, entropy = actor_critic.evaluate_actions(\n",
    "        Variable(rollout.states[:-1]).view(-1, *state_shape),\n",
    "        Variable(rollout.actions).view(-1, 1)\n",
    "    )\n",
    "\n",
    "    values = values.view(num_steps, num_envs, 1)\n",
    "    action_log_probs = action_log_probs.view(num_steps, num_envs, 1)\n",
    "    advantages = Variable(returns) - values\n",
    "\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "    action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm(actor_critic.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i_update % 100 == 0:\n",
    "        all_rewards.append(final_rewards.mean())\n",
    "        all_losses.append(loss.data[0])\n",
    "        print(\"REWARDS=> MEAN: \"+str(final_rewards.mean())+\" MAX: \"+str(final_rewards.max())+\" MEDIAN: \"+str(final_rewards.median()))\n",
    "        \n",
    "        \n",
    "    rollout.after_update()\n",
    "    \n",
    "\n",
    "# save model\n",
    "torch.save(actor_critic.state_dict(), \"actor_critic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
