{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import gym\n",
    "import gym_sokoban\n",
    "\n",
    "# import os\n",
    "# __path__=[os.path.dirname(os.path.abspath(__file__))]\n",
    "\n",
    "import multiprocessing_env as mp \n",
    "import Sokoban as sb\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnPolicy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OnPolicy, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def act(self, x, deterministic=False):\n",
    "        logit, value = self.forward(x)\n",
    "        probs = F.softmax(logit)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = probs.max(1)[1]\n",
    "        else:\n",
    "            action = probs.multinomial(1)\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def evaluate_actions(self, x, action):\n",
    "        logit, value = self.forward(x)\n",
    "        \n",
    "        probs     = F.softmax(logit)\n",
    "        log_probs = F.log_softmax(logit)\n",
    "        \n",
    "        action_log_probs = log_probs.gather(1, action)\n",
    "        entropy = -(probs * log_probs).sum(1).mean()\n",
    "        \n",
    "        return logit, action_log_probs, value, entropy\n",
    "\n",
    "class ActorCritic(OnPolicy):\n",
    "    def __init__(self, in_shape, num_actions):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_shape[0], 32, kernel_size=8, stride=8),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.critic  = nn.Linear(128, 1)\n",
    "        self.actor   = nn.Linear(128, num_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        logit = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logit, value\n",
    "    \n",
    "    def feature_size(self):\n",
    "        return self.features(autograd.Variable(torch.zeros(1, *self.in_shape))).view(1, -1).size(1)\n",
    "\n",
    "class RolloutStorage(object):\n",
    "    def __init__(self, num_steps, num_envs, state_shape):\n",
    "        self.num_steps = num_steps\n",
    "        self.num_envs  = num_envs\n",
    "        self.states  = torch.zeros(num_steps + 1, num_envs, *state_shape)\n",
    "        self.rewards = torch.zeros(num_steps,     num_envs, 1)\n",
    "        self.masks   = torch.ones(num_steps  + 1, num_envs, 1)\n",
    "        self.actions = torch.zeros(num_steps,     num_envs, 1).long()\n",
    "        self.use_cuda = False\n",
    "            \n",
    "    def cuda(self):\n",
    "        self.use_cuda  = True\n",
    "        self.states    = self.states.cuda()\n",
    "        self.rewards   = self.rewards.cuda()\n",
    "        self.masks     = self.masks.cuda()\n",
    "        self.actions   = self.actions.cuda()\n",
    "        \n",
    "    def insert(self, step, state, action, reward, mask):\n",
    "        self.states[step + 1].copy_(state)\n",
    "        self.actions[step].copy_(action)\n",
    "        self.rewards[step].copy_(reward)\n",
    "        self.masks[step + 1].copy_(mask)\n",
    "        \n",
    "    def after_update(self):\n",
    "        self.states[0].copy_(self.states[-1])\n",
    "        self.masks[0].copy_(self.masks[-1])\n",
    "        \n",
    "    def compute_returns(self, next_value, gamma):\n",
    "        returns   = torch.zeros(self.num_steps + 1, self.num_envs, 1)\n",
    "        if self.use_cuda:\n",
    "            returns = returns.cuda()\n",
    "        returns[-1] = next_value\n",
    "        for step in reversed(range(self.num_steps)):\n",
    "            returns[step] = returns[step + 1] * gamma * self.masks[step + 1] + self.rewards[step]\n",
    "        return returns[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 160, 160)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "$ Torch: not enough memory: you tried to allocate 4GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.c:218",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b6ef2120d0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     logit, action_log_probs, values, entropy = actor_critic.evaluate_actions(\n\u001b[1;32m     77\u001b[0m         \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mstate_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9479879b47f8>\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, x, action)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlogit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprobs\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-9479879b47f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/virtualenvs/pyprojects/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: $ Torch: not enough memory: you tried to allocate 4GB. Buy new RAM! at /pytorch/aten/src/TH/THGeneral.c:218"
     ]
    }
   ],
   "source": [
    "num_envs = 2\n",
    "\n",
    "def make_env():\n",
    "    def _thunk():\n",
    "        env = sb.SokobanEnv()\n",
    "        return env\n",
    "\n",
    "    return _thunk\n",
    "\n",
    "envs = [make_env() for i in range(num_envs)]\n",
    "envs = mp.SubprocVecEnv(envs)\n",
    "\n",
    "state_shape = envs.observation_space.shape\n",
    "\n",
    "#a2c hyperparams:\n",
    "gamma = 0.99\n",
    "entropy_coef = 0.01\n",
    "value_loss_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "num_steps = 120\n",
    "num_batch = int(10e5)\n",
    "\n",
    "#rmsprop hyperparams:\n",
    "lr    = 7e-4\n",
    "eps   = 1e-5\n",
    "alpha = 0.99\n",
    "\n",
    "print(envs.observation_space.shape)\n",
    "#Init a2c and rmsprop\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)\n",
    "optimizer = optim.RMSprop(actor_critic.parameters(), lr, eps=eps, alpha=alpha)\n",
    "    \n",
    "if USE_CUDA:\n",
    "    actor_critic = actor_critic.cuda()\n",
    "\n",
    "rollout = RolloutStorage(num_steps, num_envs, envs.observation_space.shape)\n",
    "# rollout.cuda()\n",
    "\n",
    "all_rewards = []\n",
    "all_losses  = []\n",
    "\n",
    "state = envs.reset()\n",
    "state = torch.FloatTensor(np.float32(state))\n",
    "\n",
    "rollout.states[0].copy_(state)\n",
    "\n",
    "episode_rewards = torch.zeros(num_envs, 1)\n",
    "final_rewards   = torch.zeros(num_envs, 1)\n",
    "\n",
    "for i_update in range(num_batch):\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        action = actor_critic.act(Variable(state))\n",
    "        next_state, reward, done, _ = envs.step(action.squeeze(1).cpu().data.numpy())\n",
    "\n",
    "        reward = torch.FloatTensor(reward).unsqueeze(1)\n",
    "        episode_rewards += reward\n",
    "        masks = torch.FloatTensor(1-np.array(done)).unsqueeze(1)\n",
    "        final_rewards *= masks\n",
    "        final_rewards += (1-masks) * episode_rewards\n",
    "        episode_rewards *= masks\n",
    "\n",
    "        if USE_CUDA:\n",
    "            masks = masks.cuda()\n",
    "\n",
    "        state = torch.FloatTensor(np.float32(next_state))\n",
    "        rollout.insert(step, state, action.data, reward, masks)\n",
    "\n",
    "\n",
    "    _, next_value = actor_critic(Variable(rollout.states[-1], requires_grad=True))\n",
    "    with torch.no_grad():\n",
    "        next_value = next_value.data\n",
    "\n",
    "    returns = rollout.compute_returns(next_value, gamma)\n",
    "\n",
    "    logit, action_log_probs, values, entropy = actor_critic.evaluate_actions(\n",
    "        Variable(rollout.states[:-1]).view(-1, *state_shape),\n",
    "        Variable(rollout.actions).view(-1, 1)\n",
    "    )\n",
    "\n",
    "    values = values.view(num_steps, num_envs, 1)\n",
    "    action_log_probs = action_log_probs.view(num_steps, num_envs, 1)\n",
    "    advantages = Variable(returns) - values\n",
    "\n",
    "    value_loss = advantages.pow(2).mean()\n",
    "    action_loss = -(Variable(advantages.data) * action_log_probs).mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss = value_loss * value_loss_coef + action_loss - entropy * entropy_coef\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm(actor_critic.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i_update % 100 == 0:\n",
    "        all_rewards.append(final_rewards.mean())\n",
    "        all_losses.append(loss.data[0])\n",
    "        print(\"REWARDS=> MEAN: \"+str(final_rewards.mean())+\" MAX: \"+str(final_rewards.max())+\" MEDIAN: \"+str(final_rewards.median()))\n",
    "        \n",
    "        \n",
    "    rollout.after_update()\n",
    "    \n",
    "\n",
    "# save model\n",
    "torch.save(actor_critic.state_dict(), \"actor_critic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
